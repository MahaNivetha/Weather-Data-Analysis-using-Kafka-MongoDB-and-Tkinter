{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;\f1\fnil\fcharset0 Menlo-Bold;\f2\fnil\fcharset0 Monaco;
\f3\fnil\fcharset0 HelveticaNeue;\f4\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red219\green39\blue218;\red86\green32\blue244;
\red57\green192\blue38;\red255\green255\blue255;\red0\green0\blue0;\red231\green231\blue237;\red39\green40\blue50;
\red199\green203\blue211;\red200\green20\blue201;\red64\green11\blue217;\red47\green180\blue29;}
{\*\expandedcolortbl;;\csgray\c0;\cssrgb\c89513\c29736\c88485;\cssrgb\c41681\c25958\c96648;
\cssrgb\c25706\c77963\c19557;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c0\c0;\cssrgb\c92549\c92549\c94510;\cssrgb\c20392\c20784\c25490;
\cssrgb\c81961\c83529\c85882;\cssrgb\c83397\c23074\c82666;\cssrgb\c32309\c18666\c88229;\cssrgb\c20241\c73898\c14950;}
\paperw11900\paperh16840\margl1440\margr1440\vieww29200\viewh17840\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \CocoaLigature0 \
\
Last login: Sat Dec 16 15:13:04 on ttys003\
(base) mahanivethakannappan@Mahanivethas-MacBook-Air ~ % spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\
\
23/12/16 15:16:13 WARN Utils: Your hostname, Mahanivethas-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.29.67 instead (on interface en0)\
23/12/16 15:16:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\
:: loading settings :: url = jar:file:/Users/mahanivethakannappan/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\
Ivy Default Cache set to: /Users/mahanivethakannappan/.ivy2/cache\
The jars for the packages stored in: /Users/mahanivethakannappan/.ivy2/jars\
org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\
:: resolving dependencies :: org.apache.spark#spark-submit-parent-5ca10815-5d97-4892-8962-e74b2777da6d;1.0\
	confs: [default]\
	found org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\
	found org.mongodb#mongodb-driver-sync;4.0.5 in central\
	found org.mongodb#bson;4.0.5 in central\
	found org.mongodb#mongodb-driver-core;4.0.5 in central\
:: resolution report :: resolve 991ms :: artifacts dl 37ms\
	:: modules in use:\
	org.mongodb#bson;4.0.5 from central in [default]\
	org.mongodb#mongodb-driver-core;4.0.5 from central in [default]\
	org.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\
	org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\
	---------------------------------------------------------------------\
	|                  |            modules            ||   artifacts   |\
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\
	---------------------------------------------------------------------\
	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\
	---------------------------------------------------------------------\
:: retrieving :: org.apache.spark#spark-submit-parent-5ca10815-5d97-4892-8962-e74b2777da6d\
	confs: [default]\
	0 artifacts copied, 4 already retrieved (0kB/31ms)\
23/12/16 15:16:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\
Setting default log level to "WARN".\
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\
23/12/16 15:16:33 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\
Spark context Web UI available at http://192.168.29.67:4041\
Spark context available as 'sc' (master = local[*], app id = local-1702719994069).\
Spark session available as 'spark'.\
23/12/16 15:16:44 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\
Welcome to\
      ____              __\
     / __/__  ___ _____/ /__\
    _\\ \\/ _ \\/ _ `/ __/  '_/\
   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.0\
      /_/\
         \
Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 20)\
Type in expressions to have them evaluated.\
Type :help for more information.\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 import org.apache.spark.sql.SparkSession\
import org.apache.spark.sql.SparkSession\
\
\cf3 scala> \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf2 val spark = SparkSession.builder .appName("MongoDBSparkAnalysis") .config("spark.mongodb.input.uri", "mongodb://localhost:27017/ .config("spark.mongodb.output.uri", "mongodb://localhost:27017/ .getOrCreate()\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 val spark = SparkSession.builder\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 spark
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession.Builder
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession$Builder@2be6103f\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .appName("MongoDBSparkAnalysis")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res0
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession.Builder
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession$Builder@2be6103f\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .config("spark.mongodb.input.uri", "mongodb://localhost:27017/weather_db.weather_collections")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res1
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession.Builder
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession$Builder@2be6103f\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .config("spark.mongodb.output.uri", "mongodb://localhost:27017/weather_db.weather_collections")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res2
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession.Builder
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession$Builder@2be6103f\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .getOrCreate()\
23/12/16 15:16:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res3
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession@3c844476\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val df = spark\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 df
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession.Builder
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession$Builder@2be6103f\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 val spark = df.getOrCreate()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 spark
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession@3c844476\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 val weatherDF = spark.read\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 weatherDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrameReader
\f0\b0 \cf2  = org.apache.spark.sql.DataFrameReader@5ed85a18\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .format("com.mongodb.spark.sql.DefaultSource")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res4
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrameReader
\f0\b0 \cf2  = org.apache.spark.sql.DataFrameReader@5ed85a18\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .option("uri", "mongodb://localhost:27017/weather_db.weather_collections")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res5
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrameReader
\f0\b0 \cf2  = org.apache.spark.sql.DataFrameReader@5ed85a18\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .load()\
23/12/16 15:17:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res6
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 val weatherData = weatherDF.load()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 weatherData
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 weatherData.printSchema()\
root\
 |-- _id: struct (nullable = true)\
 |    |-- oid: string (nullable = true)\
 |-- location: struct (nullable = true)\
 |    |-- lat: double (nullable = true)\
 |    |-- lon: double (nullable = true)\
 |-- timelines: struct (nullable = true)\
 |    |-- minutely: array (nullable = true)\
 |    |    |-- element: struct (containsNull = true)\
 |    |    |    |-- time: string (nullable = true)\
 |    |    |    |-- values: struct (nullable = true)\
 |    |    |    |    |-- cloudBase: double (nullable = true)\
 |    |    |    |    |-- cloudCeiling: double (nullable = true)\
 |    |    |    |    |-- cloudCover: double (nullable = true)\
 |    |    |    |    |-- dewPoint: double (nullable = true)\
 |    |    |    |    |-- freezingRainIntensity: integer (nullable = true)\
 |    |    |    |    |-- humidity: double (nullable = true)\
 |    |    |    |    |-- precipitationProbability: integer (nullable = true)\
 |    |    |    |    |-- pressureSurfaceLevel: double (nullable = true)\
 |    |    |    |    |-- rainIntensity: integer (nullable = true)\
 |    |    |    |    |-- sleetIntensity: integer (nullable = true)\
 |    |    |    |    |-- snowIntensity: integer (nullable = true)\
 |    |    |    |    |-- temperature: double (nullable = true)\
 |    |    |    |    |-- temperatureApparent: double (nullable = true)\
 |    |    |    |    |-- uvHealthConcern: integer (nullable = true)\
 |    |    |    |    |-- uvIndex: integer (nullable = true)\
 |    |    |    |    |-- visibility: integer (nullable = true)\
 |    |    |    |    |-- weatherCode: integer (nullable = true)\
 |    |    |    |    |-- windDirection: double (nullable = true)\
 |    |    |    |    |-- windGust: double (nullable = true)\
 |    |    |    |    |-- windSpeed: double (nullable = true)\
 |    |-- hourly: array (nullable = true)\
 |    |    |-- element: struct (containsNull = true)\
 |    |    |    |-- time: string (nullable = true)\
 |    |    |    |-- values: struct (nullable = true)\
 |    |    |    |    |-- cloudBase: double (nullable = true)\
 |    |    |    |    |-- cloudCeiling: double (nullable = true)\
 |    |    |    |    |-- cloudCover: double (nullable = true)\
 |    |    |    |    |-- dewPoint: double (nullable = true)\
 |    |    |    |    |-- evapotranspiration: double (nullable = true)\
 |    |    |    |    |-- freezingRainIntensity: integer (nullable = true)\
 |    |    |    |    |-- humidity: double (nullable = true)\
 |    |    |    |    |-- iceAccumulation: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationLwe: integer (nullable = true)\
 |    |    |    |    |-- precipitationProbability: integer (nullable = true)\
 |    |    |    |    |-- pressureSurfaceLevel: double (nullable = true)\
 |    |    |    |    |-- rainAccumulation: double (nullable = true)\
 |    |    |    |    |-- rainAccumulationLwe: double (nullable = true)\
 |    |    |    |    |-- rainIntensity: double (nullable = true)\
 |    |    |    |    |-- sleetAccumulation: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationLwe: integer (nullable = true)\
 |    |    |    |    |-- sleetIntensity: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulation: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationLwe: integer (nullable = true)\
 |    |    |    |    |-- snowIntensity: integer (nullable = true)\
 |    |    |    |    |-- temperature: double (nullable = true)\
 |    |    |    |    |-- temperatureApparent: double (nullable = true)\
 |    |    |    |    |-- uvHealthConcern: integer (nullable = true)\
 |    |    |    |    |-- uvIndex: integer (nullable = true)\
 |    |    |    |    |-- visibility: double (nullable = true)\
 |    |    |    |    |-- weatherCode: integer (nullable = true)\
 |    |    |    |    |-- windDirection: double (nullable = true)\
 |    |    |    |    |-- windGust: double (nullable = true)\
 |    |    |    |    |-- windSpeed: double (nullable = true)\
 |    |-- daily: array (nullable = true)\
 |    |    |-- element: struct (containsNull = true)\
 |    |    |    |-- time: string (nullable = true)\
 |    |    |    |-- values: struct (nullable = true)\
 |    |    |    |    |-- cloudBaseAvg: double (nullable = true)\
 |    |    |    |    |-- cloudBaseMax: double (nullable = true)\
 |    |    |    |    |-- cloudBaseMin: double (nullable = true)\
 |    |    |    |    |-- cloudCeilingAvg: double (nullable = true)\
 |    |    |    |    |-- cloudCeilingMax: double (nullable = true)\
 |    |    |    |    |-- cloudCeilingMin: integer (nullable = true)\
 |    |    |    |    |-- cloudCoverAvg: double (nullable = true)\
 |    |    |    |    |-- cloudCoverMax: double (nullable = true)\
 |    |    |    |    |-- cloudCoverMin: double (nullable = true)\
 |    |    |    |    |-- dewPointAvg: double (nullable = true)\
 |    |    |    |    |-- dewPointMax: double (nullable = true)\
 |    |    |    |    |-- dewPointMin: double (nullable = true)\
 |    |    |    |    |-- evapotranspirationAvg: double (nullable = true)\
 |    |    |    |    |-- evapotranspirationMax: double (nullable = true)\
 |    |    |    |    |-- evapotranspirationMin: double (nullable = true)\
 |    |    |    |    |-- evapotranspirationSum: double (nullable = true)\
 |    |    |    |    |-- freezingRainIntensityAvg: integer (nullable = true)\
 |    |    |    |    |-- freezingRainIntensityMax: integer (nullable = true)\
 |    |    |    |    |-- freezingRainIntensityMin: integer (nullable = true)\
 |    |    |    |    |-- humidityAvg: double (nullable = true)\
 |    |    |    |    |-- humidityMax: double (nullable = true)\
 |    |    |    |    |-- humidityMin: double (nullable = true)\
 |    |    |    |    |-- iceAccumulationAvg: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationLweAvg: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationLweMax: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationLweMin: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationLweSum: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationMax: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationMin: integer (nullable = true)\
 |    |    |    |    |-- iceAccumulationSum: integer (nullable = true)\
 |    |    |    |    |-- moonriseTime: string (nullable = true)\
 |    |    |    |    |-- moonsetTime: string (nullable = true)\
 |    |    |    |    |-- precipitationProbabilityAvg: double (nullable = true)\
 |    |    |    |    |-- precipitationProbabilityMax: integer (nullable = true)\
 |    |    |    |    |-- precipitationProbabilityMin: integer (nullable = true)\
 |    |    |    |    |-- pressureSurfaceLevelAvg: double (nullable = true)\
 |    |    |    |    |-- pressureSurfaceLevelMax: double (nullable = true)\
 |    |    |    |    |-- pressureSurfaceLevelMin: double (nullable = true)\
 |    |    |    |    |-- rainAccumulationAvg: double (nullable = true)\
 |    |    |    |    |-- rainAccumulationLweAvg: double (nullable = true)\
 |    |    |    |    |-- rainAccumulationLweMax: double (nullable = true)\
 |    |    |    |    |-- rainAccumulationLweMin: integer (nullable = true)\
 |    |    |    |    |-- rainAccumulationMax: double (nullable = true)\
 |    |    |    |    |-- rainAccumulationMin: integer (nullable = true)\
 |    |    |    |    |-- rainAccumulationSum: double (nullable = true)\
 |    |    |    |    |-- rainIntensityAvg: double (nullable = true)\
 |    |    |    |    |-- rainIntensityMax: double (nullable = true)\
 |    |    |    |    |-- rainIntensityMin: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationAvg: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationLweAvg: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationLweMax: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationLweMin: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationLweSum: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationMax: integer (nullable = true)\
 |    |    |    |    |-- sleetAccumulationMin: integer (nullable = true)\
 |    |    |    |    |-- sleetIntensityAvg: integer (nullable = true)\
 |    |    |    |    |-- sleetIntensityMax: integer (nullable = true)\
 |    |    |    |    |-- sleetIntensityMin: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationAvg: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationLweAvg: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationLweMax: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationLweMin: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationLweSum: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationMax: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationMin: integer (nullable = true)\
 |    |    |    |    |-- snowAccumulationSum: integer (nullable = true)\
 |    |    |    |    |-- snowIntensityAvg: integer (nullable = true)\
 |    |    |    |    |-- snowIntensityMax: integer (nullable = true)\
 |    |    |    |    |-- snowIntensityMin: integer (nullable = true)\
 |    |    |    |    |-- sunriseTime: string (nullable = true)\
 |    |    |    |    |-- sunsetTime: string (nullable = true)\
 |    |    |    |    |-- temperatureApparentAvg: double (nullable = true)\
 |    |    |    |    |-- temperatureApparentMax: double (nullable = true)\
 |    |    |    |    |-- temperatureApparentMin: double (nullable = true)\
 |    |    |    |    |-- temperatureAvg: double (nullable = true)\
 |    |    |    |    |-- temperatureMax: double (nullable = true)\
 |    |    |    |    |-- temperatureMin: double (nullable = true)\
 |    |    |    |    |-- uvHealthConcernAvg: integer (nullable = true)\
 |    |    |    |    |-- uvHealthConcernMax: integer (nullable = true)\
 |    |    |    |    |-- uvHealthConcernMin: integer (nullable = true)\
 |    |    |    |    |-- uvIndexAvg: integer (nullable = true)\
 |    |    |    |    |-- uvIndexMax: integer (nullable = true)\
 |    |    |    |    |-- uvIndexMin: integer (nullable = true)\
 |    |    |    |    |-- visibilityAvg: double (nullable = true)\
 |    |    |    |    |-- visibilityMax: double (nullable = true)\
 |    |    |    |    |-- visibilityMin: double (nullable = true)\
 |    |    |    |    |-- weatherCodeMax: integer (nullable = true)\
 |    |    |    |    |-- weatherCodeMin: integer (nullable = true)\
 |    |    |    |    |-- windDirectionAvg: double (nullable = true)\
 |    |    |    |    |-- windGustAvg: double (nullable = true)\
 |    |    |    |    |-- windGustMax: double (nullable = true)\
 |    |    |    |    |-- windGustMin: double (nullable = true)\
 |    |    |    |    |-- windSpeedAvg: double (nullable = true)\
 |    |    |    |    |-- windSpeedMax: double (nullable = true)\
 |    |    |    |    |-- windSpeedMin: double (nullable = true)\
\
\
\cf3 scala> \cf2 weatherData.select("timelines.daily.time", "timelines.daily.values.temperatureAvg")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res8
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [time: array<string>, temperatureAvg: array<double>]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .orderBy($"temperatureAvg".desc)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res9
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
\f0\b0 \cf2  = [time: array<string>, temperatureAvg: array<double>]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .limit(1)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res10
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
\f0\b0 \cf2  = [time: array<string>, temperatureAvg: array<double>]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .show()\
+--------------------+--------------------+                                     \
|                time|      temperatureAvg|\
+--------------------+--------------------+\
|[2023-12-15T00:30...|[21.89, 22.27, 21...|\
+--------------------+--------------------+\
\
\
\cf3 scala> \cf2 import org.apache.spark.sql.functions.\{explode, col\}\
import org.apache.spark.sql.functions.\{explode, col\}\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Assuming your DataFrame is named 'weatherData'\
\
\cf3 scala> \cf2 val explodedDF = weatherData.select(col("_id"),\
\cf3      | \cf2   col("location"),\
\cf3      | \cf2   explode(col("timelines.minutely")).as("minutely"))\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 explodedDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val formattedDF = explodedDF.select(\
\cf3      | \cf2   col("_id"),\
\cf3      | \cf2   col("location"),\
\cf3      | \cf2   col("minutely.time").as("time"),\
\cf3      | \cf2   col("minutely.values.cloudBase").as("cloudBase"),\
\cf3      | \cf2   col("minutely.values.cloudCeiling").as("cloudCeiling"),\
\cf3      | \cf2   col("minutely.values.cloudCover").as("cloudCover"),\
\cf3      | \cf2   col("minutely.values.dewPoint").as("dewPoint"),\
\cf3      | \cf2   col("minutely.values.freezingRainIntensity").as("freezingRainIntensity"),\
\cf3      | \cf2   col("minutely.values.humidity").as("humidity"),\
\cf3      | \cf2   col("minutely.values.precipitationProbability").as("precipitationProbability"),\
\cf3      | \cf2   col("minutely.values.pressureSurfaceLevel").as("pressureSurfaceLevel"),\
\cf3      | \cf2   col("minutely.values.rainIntensity").as("rainIntensity"),\
\cf3      | \cf2   col("minutely.values.sleetIntensity").as("sleetIntensity"),\
\cf3      | \cf2   col("minutely.values.snowIntensity").as("snowIntensity"),\
\cf3      | \cf2   col("minutely.values.temperature").as("temperature"),\
\cf3      | \cf2   col("minutely.values.temperatureApparent").as("temperatureApparent"),\
\cf3      | \cf2   col("minutely.values.uvHealthConcern").as("uvHealthConcern"),\
\cf3      | \cf2   col("minutely.values.uvIndex").as("uvIndex"),\
\cf3      | \cf2   col("minutely.values.visibility").as("visibility"),\
\cf3      | \cf2   col("minutely.values.weatherCode").as("weatherCode"),\
\cf3      | \cf2   col("minutely.values.windDirection").as("windDirection"),\
\cf3      | \cf2   col("minutely.values.windGust").as("windGust"),\
\cf3      | \cf2   col("minutely.values.windSpeed").as("windSpeed")\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 formattedDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 21 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 formattedDF.show(false) \
\
\
\
\
+--------------------------+------------------+--------------------+---------+------------+----------+--------+---------------------+--------+------------------------+--------------------+-------------+--------------+-------------+-----------+-------------------+---------------+-------+----------+-----------+-------------+--------+---------+\
|_id                       |location          |time                |cloudBase|cloudCeiling|cloudCover|dewPoint|freezingRainIntensity|humidity|precipitationProbability|pressureSurfaceLevel|rainIntensity|sleetIntensity|snowIntensity|temperature|temperatureApparent|uvHealthConcern|uvIndex|visibility|weatherCode|windDirection|windGust|windSpeed|\
+--------------------------+------------------+--------------------+---------+------------+----------+--------+---------------------+--------+------------------------+--------------------+-------------+--------------+-------------+-----------+-------------------+---------------+-------+----------+-----------+-------------+--------+---------+\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T10:59:00Z|0.55     |0.55        |75.0      |19.69   |0                    |78.0    |0                       |910.81              |0            |0             |0            |25.81      |25.81              |0              |0      |16        |1001       |96.13        |3.19    |2.13     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:00:00Z|1.07     |10.37       |99.22     |16.72   |0                    |56.45   |0                       |910.81              |0            |0             |0            |26.04      |26.04              |0              |0      |16        |1001       |87.83        |7.64    |3.88     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:01:00Z|1.07     |10.37       |99.23     |16.73   |0                    |56.63   |0                       |910.82              |0            |0             |0            |26.01      |26.01              |0              |0      |16        |1001       |87.83        |7.64    |3.89     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:02:00Z|1.07     |10.37       |99.24     |16.75   |0                    |56.8    |0                       |910.83              |0            |0             |0            |25.98      |25.98              |0              |0      |16        |1001       |87.83        |7.65    |3.9      |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:03:00Z|1.07     |10.37       |99.26     |16.76   |0                    |56.98   |0                       |910.84              |0            |0             |0            |25.95      |25.95              |0              |0      |16        |1001       |87.83        |7.65    |3.91     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:04:00Z|1.07     |10.37       |99.27     |16.78   |0                    |57.15   |0                       |910.85              |0            |0             |0            |25.92      |25.92              |0              |0      |16        |1001       |87.83        |7.65    |3.91     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:05:00Z|1.07     |10.37       |99.28     |16.79   |0                    |57.32   |0                       |910.86              |0            |0             |0            |25.89      |25.89              |0              |0      |16        |1001       |87.83        |7.65    |3.92     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:06:00Z|1.07     |10.37       |99.3      |16.81   |0                    |57.5    |0                       |910.87              |0            |0             |0            |25.85      |25.85              |0              |0      |16        |1001       |87.83        |7.66    |3.93     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:07:00Z|1.07     |10.37       |99.31     |16.82   |0                    |57.67   |0                       |910.88              |0            |0             |0            |25.82      |25.82              |0              |0      |16        |1001       |87.83        |7.66    |3.94     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:08:00Z|1.07     |10.37       |99.32     |16.84   |0                    |57.85   |0                       |910.89              |0            |0             |0            |25.79      |25.79              |0              |0      |16        |1001       |87.83        |7.66    |3.94     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:09:00Z|1.07     |10.37       |99.34     |16.85   |0                    |58.02   |0                       |910.9               |0            |0             |0            |25.76      |25.76              |0              |0      |16        |1001       |87.83        |7.66    |3.95     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:10:00Z|1.07     |10.37       |99.35     |16.87   |0                    |58.19   |0                       |910.91              |0            |0             |0            |25.73      |25.73              |0              |0      |16        |1001       |87.83        |7.66    |3.96     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:11:00Z|1.07     |10.37       |99.36     |16.88   |0                    |58.37   |0                       |910.92              |0            |0             |0            |25.7       |25.7               |0              |0      |16        |1001       |87.83        |7.67    |3.97     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:12:00Z|1.07     |10.37       |99.38     |16.9    |0                    |58.54   |0                       |910.93              |0            |0             |0            |25.66      |25.66              |0              |0      |16        |1001       |87.83        |7.67    |3.97     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:13:00Z|1.07     |10.37       |99.39     |16.91   |0                    |58.71   |0                       |910.93              |0            |0             |0            |25.63      |25.63              |0              |0      |16        |1001       |87.83        |7.67    |3.98     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:14:00Z|1.07     |10.37       |99.4      |16.93   |0                    |58.89   |0                       |910.94              |0            |0             |0            |25.6       |25.6               |0              |0      |16        |1001       |87.83        |7.67    |3.99     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:15:00Z|1.07     |10.37       |99.41     |16.94   |0                    |59.06   |0                       |910.95              |0            |0             |0            |25.57      |25.57              |0              |0      |16        |1001       |87.83        |7.67    |4.0      |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:16:00Z|1.07     |10.37       |99.43     |16.96   |0                    |59.24   |0                       |910.96              |0            |0             |0            |25.54      |25.54              |0              |0      |16        |1001       |87.83        |7.68    |4.0      |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:17:00Z|1.07     |10.37       |99.44     |16.97   |0                    |59.41   |0                       |910.97              |0            |0             |0            |25.51      |25.51              |0              |0      |16        |1001       |87.83        |7.68    |4.01     |\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:18:00Z|1.07     |10.37       |99.45     |16.99   |0                    |59.58   |0                       |910.98              |0            |0             |0            |25.48      |25.48              |0              |0      |16        |1001       |87.83        |7.68    |4.02     |\
+--------------------------+------------------+--------------------+---------+------------+----------+--------+---------------------+--------+------------------------+--------------------+-------------+--------------+-------------+-----------+-------------------+---------------+-------+----------+-----------+-------------+--------+---------+\
only showing top 20 rows\
\
\
\
\
\'97\'97\'97\'97\'97\
\cf3 scala> \cf2 import org.apache.spark.sql.functions._\
import org.apache.spark.sql.functions._\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Explode the minutely array\
\
\cf3 scala> \cf2 val minutelyDF = flattenedDF.selectExpr("_id", "location", "explode(timelines.minutely.time) as minutely_time")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 minutelyDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Explode the hourly array\
\
\cf3 scala> \cf2 val hourlyDF = flattenedDF.selectExpr("_id", "location", "explode(timelines.hourly.time) as hourly_time")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 hourlyDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Explode the daily array\
\
\cf3 scala> \cf2 val dailyDF = flattenedDF.selectExpr("_id", "location", "explode(timelines.daily.time) as daily_time")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 dailyDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Perform joins to combine the results\
\
\cf3 scala> \cf2 val resultDF = minutelyDF\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 resultDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .join(hourlyDF, Seq("_id", "location"), "outer")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res36
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 2 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .join(dailyDF, Seq("_id", "location"), "outer")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res37
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Show the resulting DataFrame\
\
\cf3 scala> \cf2 resultDF.show()\
+--------------------+------------------+--------------------+                  \
|                 _id|          location|       minutely_time|\
+--------------------+------------------+--------------------+\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T10:59:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:00:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:01:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:02:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:03:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:04:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:05:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:06:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:07:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:08:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:09:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:10:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:11:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:12:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:13:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:14:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:15:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:16:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:17:00Z|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:18:00Z|\
+--------------------+------------------+--------------------+\
only showing top 20 rows\
\
\cf3 scala> \cf2 import org.apache.spark.sql.functions._\
import org.apache.spark.sql.functions._\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val resultDF = flattenedDF.select(\
\cf3      | \cf2   col("_id"),\
\cf3      | \cf2   col("location"),\
\cf3      | \cf2   expr("timelines.minutely.time[0]").as("minutely_time"),\
\cf3      | \cf2   expr("timelines.hourly.time[0]").as("hourly_time"),\
\cf3      | \cf2   expr("timelines.daily.time[0]").as("daily_time")\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 resultDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 resultDF.show()\
+--------------------+------------------+--------------------+--------------------+--------------------+\
|                 _id|          location|       minutely_time|         hourly_time|          daily_time|\
+--------------------+------------------+--------------------+--------------------+--------------------+\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T10:59:00Z|2023-12-15T10:00:00Z|2023-12-15T00:30:00Z|\
+--------------------+------------------+--------------------+--------------------+--------------------+\
\cf3 scala> \cf2 val resultDF = flattenedDF.select(\
\cf3      | \cf2   col("_id"),\
\cf3      | \cf2   col("location"),\
\cf3      | \cf2   expr("timelines.minutely.time[10]").as("minutely_time"),\
\cf3      | \cf2   expr("timelines.hourly.time[10]").as("hourly_time"),\
\cf3      | \cf2   expr("timelines.daily.time[10]").as("daily_time")\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 resultDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 resultDF.show()\
+--------------------+------------------+--------------------+--------------------+----------+\
|                 _id|          location|       minutely_time|         hourly_time|daily_time|\
+--------------------+------------------+--------------------+--------------------+----------+\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T11:09:00Z|2023-12-15T20:00:00Z|      NULL|\
+--------------------+------------------+--------------------+--------------------+----------+\
\
\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
Corrected \
\
\'97\'97\'97\'97\'97\'97\'97\
\cf3 scala> \cf2 val explodedMinuteliesDF = resultDF.selectExpr(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "explode(timelines.minutely.time) as minutely_time"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 explodedMinuteliesDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val explodedHourliesDF = resultDF.selectExpr(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "explode(timelines.hourly.time) as hourly_time"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 explodedHourliesDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val explodedDailiesDF = resultDF.selectExpr(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "explode(timelines.daily.time) as daily_time"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 explodedDailiesDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 explodedMinuteliesDF.show(truncate = false)\
+--------------------------+------------------+--------------------+\
|_id                       |location          |minutely_time       |\
+--------------------------+------------------+--------------------+\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T10:59:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:01:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:02:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:03:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:04:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:05:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:06:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:07:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:08:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:09:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:10:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:11:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:12:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:13:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:14:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:15:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:16:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:17:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:18:00Z|\
+--------------------------+------------------+--------------------+\
only showing top 20 rows\
\
\
\cf3 scala> \cf2 explodedHourliesDF.show(truncate = false)\
+--------------------------+------------------+--------------------+            \
|_id                       |location          |hourly_time         |\
+--------------------------+------------------+--------------------+\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T10:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T11:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T12:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T13:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T14:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T15:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T16:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T17:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T18:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T19:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T20:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T21:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T22:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T23:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T00:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T01:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T02:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T03:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T04:00:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T05:00:00Z|\
+--------------------------+------------------+--------------------+\
only showing top 20 rows\
\
\
\cf3 scala> \cf2 explodedDailiesDF.show(truncate = false)\
+--------------------------+------------------+--------------------+\
|_id                       |location          |daily_time          |\
+--------------------------+------------------+--------------------+\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-15T00:30:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-16T00:30:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-17T00:30:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-18T00:30:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-19T00:30:00Z|\
|\{657c31b2b8dc5929a778b295\}|\{12.9716, 77.5946\}|2023-12-20T00:30:00Z|\
+--------------------------+------------------+--------------------+\
\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\
Moonrise moon set \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\cf3 scala> \cf2 val explodedTimelinesDF = resultDF.selectExpr(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "explode(timelines.daily) as daily_data"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 explodedTimelinesDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val extractedMoonTimesDF = explodedTimelinesDF.select(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "daily_data.time",\
\cf3      | \cf2   "daily_data.values.moonriseTime",\
\cf3      | \cf2   "daily_data.values.moonsetTime"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 extractedMoonTimesDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Now, let's calculate the difference in seconds\
\
\cf3 scala> \cf2 val diffDF = extractedMoonTimesDF.selectExpr(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "time",\
\cf3      | \cf2   "moonriseTime",\
\cf3      | \cf2   "moonsetTime",\
\cf3      | \cf2   "unix_timestamp(moonsetTime, 'yyyy-MM-dd HH:mm:ss') - unix_timestamp(moonriseTime, 'yyyy-MM-dd HH:mm:ss') as difference"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 diffDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 4 more fields]\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 diffDF.show()\
+--------------------+------------------+--------------------+--------------------+--------------------+----------+\
|                 _id|          location|                time|        moonriseTime|         moonsetTime|difference|\
+--------------------+------------------+--------------------+--------------------+--------------------+----------+\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T00:30:00Z|2023-12-15T03:10:22Z|2023-12-15T14:48:48Z|      NULL|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-16T00:30:00Z|2023-12-16T04:07:05Z|2023-12-16T15:52:15Z|      NULL|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-17T00:30:00Z|2023-12-17T04:58:55Z|2023-12-17T16:53:50Z|      NULL|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-18T00:30:00Z|2023-12-18T05:47:33Z|2023-12-18T17:52:30Z|      NULL|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-19T00:30:00Z|2023-12-19T06:32:23Z|2023-12-19T18:49:27Z|      NULL|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-20T00:30:00Z|2023-12-20T07:14:35Z|2023-12-20T19:44:56Z|      NULL|\
+--------------------+------------------+--------------------+--------------------+--------------------+----------+\
\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\pardeftab720\partightenfactor0

\f2\fs28 \cf6 \cb7 \expnd0\expndtw0\kerning0
\CocoaLigature1  Calculate daily summaries\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf3 \cb1 \kerning1\expnd0\expndtw0 \CocoaLigature0 scala> \cf2 import org.apache.spark.sql.functions._\
import org.apache.spark.sql.functions._\
\
\cf3 scala> \cf2 val explodedDailyDF = weatherData.selectExpr(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "explode(timelines.daily) as daily_data"\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 explodedDailyDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 1 more field]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Extract relevant columns\
\
\cf3 scala> \cf2 val extractedDailyMetricsDF = explodedDailyDF.select(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "daily_data.time",\
\cf3      | \cf2   "daily_data.values.temperatureAvg",\
\cf3      | \cf2   "daily_data.values.temperatureMax",\
\cf3      | \cf2   "daily_data.values.temperatureMin",\
\cf3      | \cf2   "daily_data.values.humidityAvg",\
\cf3      | \cf2   "daily_data.values.humidityMax",\
\cf3      | \cf2   "daily_data.values.humidityMin"\
\cf3      | \cf2   // Add other relevant columns here\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 extractedDailyMetricsDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 7 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 // Calculate daily summaries\
\
\cf3 scala> \cf2 val dailySummariesDF = extractedDailyMetricsDF.groupBy(\
\cf3      | \cf2   "_id",\
\cf3      | \cf2   "location",\
\cf3      | \cf2   "time" // Use the correct column name "time"\
\cf3      | \cf2 ).agg(\
\cf3      | \cf2   avg("temperatureAvg").alias("avgTemperature"),\
\cf3      | \cf2   max("temperatureMax").alias("maxTemperature"),\
\cf3      | \cf2   min("temperatureMin").alias("minTemperature"),\
\cf3      | \cf2   avg("humidityAvg").alias("avgHumidity"),\
\cf3      | \cf2   max("humidityMax").alias("maxHumidity"),\
\cf3      | \cf2   min("humidityMin").alias("minHumidity")\
\cf3      | \cf2   // Add other aggregation functions for additional metrics\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 dailySummariesDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 7 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Show the resulting DataFrame\
\
\cf3 scala> \cf2 dailySummariesDF.show()\
+--------------------+------------------+--------------------+--------------+--------------+--------------+-----------+-----------+-----------+\
|                 _id|          location|                time|avgTemperature|maxTemperature|minTemperature|avgHumidity|maxHumidity|minHumidity|\
+--------------------+------------------+--------------------+--------------+--------------+--------------+-----------+-----------+-----------+\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-16T00:30:00Z|         22.27|         27.26|         18.31|      78.09|      92.17|       54.3|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-20T00:30:00Z|         22.53|         25.87|         17.02|      55.59|      82.41|      44.15|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-18T00:30:00Z|         20.87|         26.27|         16.86|      69.49|      95.82|      43.89|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-17T00:30:00Z|          21.5|         27.58|         17.04|      76.54|       96.5|      47.36|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T00:30:00Z|         21.89|         26.63|         18.38|      84.45|       97.0|      56.45|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-19T00:30:00Z|          21.1|         26.14|          17.3|      58.95|      81.36|      29.72|\
+--------------------+------------------+--------------------+--------------+--------------+--------------+-----------+-----------+-----------+\
\'97\'97\'97\'97\'97\'97\'97\
\
\pard\pardeftab720\partightenfactor0

\f3\fs32 \cf8 \cb9 \expnd0\expndtw0\kerning0
\CocoaLigature1 Extreme Weather Events:\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\pardeftab720\partightenfactor0

\f0\fs22 \cf2 \cb1 \kerning1\expnd0\expndtw0 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
 \cf3 scala> \cf2 import org.apache.spark.sql.functions._\
import org.apache.spark.sql.functions._\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 import org.apache.spark.sql.SparkSession\
import org.apache.spark.sql.SparkSession\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Create a Spark session\
\
\cf3 scala> \cf2 val spark = SparkSession.builder.appName("WeatherAnalysis").getOrCreate()\
23/12/16 23:20:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 spark
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.SparkSession
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession@382b008d\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Assuming you have loaded the weather data into a DataFrame named 'weatherData'\
\
\cf3 scala> \cf2 // Please replace "temperatureMax" with the actual column name containing maximum temperature\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Select relevant columns\
\
\cf3 scala> \cf2 val temperatureData = weatherData.select(\
\cf3      | \cf2   col("_id"),\
\cf3      | \cf2   col("location"),\
\cf3      | \cf2   explode(col("timelines.daily")).as("daily_data")\
\cf3      | \cf2 ).select(\
\cf3      | \cf2   col("_id"),\
\cf3      | \cf2   col("location"),\
\cf3      | \cf2   col("daily_data.time"),\
\cf3      | \cf2   col("daily_data.values.temperatureMax").as("maxTemperature")\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 temperatureData
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 2 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\cf3 scala> \cf2 // Adjusted threshold\
\
\cf3 scala> \cf2 val newExtremeTemperatureThreshold = 25.0\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 newExtremeTemperatureThreshold
\f0\b0 \cf2 : 
\f1\b \cf5 Double
\f0\b0 \cf2  = 25.0\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Identify days with extreme temperature\
\
\cf3 scala> \cf2 val newExtremeTemperatureDays = temperatureData.filter(col("maxTemperature") > newExtremeTemperatureThreshold)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 newExtremeTemperatureDays
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
\f0\b0 \cf2  = [_id: struct<oid: string>, location: struct<lat: double, lon: double> ... 2 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Show the resulting DataFrame\
\
\cf3 scala> \cf2 newExtremeTemperatureDays.show()\
+--------------------+------------------+--------------------+--------------+   \
|                 _id|          location|                time|maxTemperature|\
+--------------------+------------------+--------------------+--------------+\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-15T00:30:00Z|         26.63|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-16T00:30:00Z|         27.26|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-17T00:30:00Z|         27.58|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-18T00:30:00Z|         26.27|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-19T00:30:00Z|         26.14|\
|\{657c31b2b8dc5929...|\{12.9716, 77.5946\}|2023-12-20T00:30:00Z|         25.87|\
+--------------------+------------------+--------------------+--------------+\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
CLOUD COVER \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\cf3 scala> \cf2 import org.apache.spark.sql.functions.col\
import org.apache.spark.sql.functions.col\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val cloudCoverAnalysisDF = weatherData.select(\
\cf3      | \cf2   col("timelines.minutely.time").alias("time"),\
\cf3      | \cf2   col("timelines.minutely.values.cloudCover").alias("cloudCover"),\
\cf3      | \cf2   col("timelines.minutely.values.temperature").alias("temperature"),\
\cf3      | \cf2   col("timelines.minutely.values.humidity").alias("humidity"),\
\cf3      | \cf2   col("timelines.minutely.values.precipitationProbability").alias("precipitationProbability")\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 cloudCoverAnalysisDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [time: array<string>, cloudCover: array<double> ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 import org.apache.spark.sql.functions.\{col, expr\}\
import org.apache.spark.sql.functions.\{col, expr\}\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val cloudCoverAnalysisDF = weatherData.select(\
\cf3      | \cf2   expr("timelines.minutely.time[0]").alias("time"),\
\cf3      | \cf2   expr("timelines.minutely.values.cloudCover[0]").alias("cloudCover"),\
\cf3      | \cf2   expr("timelines.minutely.values.temperature[0]").alias("temperature"),\
\cf3      | \cf2   expr("timelines.minutely.values.humidity[0]").alias("humidity"),\
\cf3      | \cf2   expr("timelines.minutely.values.precipitationProbability[0]").alias("precipitationProbability")\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 cloudCoverAnalysisDF
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [time: string, cloudCover: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 cloudCoverAnalysisDF.show()\
+--------------------+----------+-----------+--------+------------------------+ \
|                time|cloudCover|temperature|humidity|precipitationProbability|\
+--------------------+----------+-----------+--------+------------------------+\
|2023-12-15T10:59:00Z|      75.0|      25.81|    78.0|                       0|\
+--------------------+----------+-----------+--------+------------------------+\
\
\
\
\
===================\
Weather code \
===================\
\cf3 scala> \cf2 val maxWeatherCode = weatherData.orderBy(desc("timelines.minutely.values.precipitationProbability")).first()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 maxWeatherCode
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Row
\f0\b0 \cf2  = [[657c31b2b8dc5929a778b295],[12.9716,77.5946],[WrappedArray([2023-12-15T10:59:00Z,[0.55,0.55,75.0,19.69,0,78.0,0,910.81,0,0,0,25.81,25.81,0,0,16,1001,96.13,3.19,2.13]], [2023-12-15T11:00:00Z,[1.07,10.37,99.22,16.72,0,56.45,0,910.81,0,0,0,26.04,26.04,0,0,16,1001,87.83,7.64,3.88]], [2023-12-15T11:01:00Z,[1.07,10.37,99.23,16.73,0,56.63,0,910.82,0,0,0,26.01,26.01,0,0,16,1001,87.83,7.64,3.89]], [2023-12-15T11:02:00Z,[1.07,10.37,99.24,16.75,0,56.8,0,910.83,0,0,0,25.98,25.98,0,0,16,1001,87.83,7.65,3.9]], [2023-12-15T11:03:00Z,[1.07,10.37,99.26,16.76,0,56.98,0,910.84,0,0,0,25.95,25.95,0,0,16,1001,87.83,7.65,3.91]], [2023-12-15T11:04:00Z,[1.07,10.37,99.27,16.78,0,57.15,0,910.85,0,0,0,25.92,25.92,0,0,16,1001,87.83,7.65,3.91]], [...\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Code to find minWeatherCode\
\
\cf3 scala> \cf2 val minWeatherCode = weatherData.orderBy("timelines.minutely.values.precipitationProbability").first()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 minWeatherCode
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Row
\f0\b0 \cf2  = [[657c31b2b8dc5929a778b295],[12.9716,77.5946],[WrappedArray([2023-12-15T10:59:00Z,[0.55,0.55,75.0,19.69,0,78.0,0,910.81,0,0,0,25.81,25.81,0,0,16,1001,96.13,3.19,2.13]], [2023-12-15T11:00:00Z,[1.07,10.37,99.22,16.72,0,56.45,0,910.81,0,0,0,26.04,26.04,0,0,16,1001,87.83,7.64,3.88]], [2023-12-15T11:01:00Z,[1.07,10.37,99.23,16.73,0,56.63,0,910.82,0,0,0,26.01,26.01,0,0,16,1001,87.83,7.64,3.89]], [2023-12-15T11:02:00Z,[1.07,10.37,99.24,16.75,0,56.8,0,910.83,0,0,0,25.98,25.98,0,0,16,1001,87.83,7.65,3.9]], [2023-12-15T11:03:00Z,[1.07,10.37,99.26,16.76,0,56.98,0,910.84,0,0,0,25.95,25.95,0,0,16,1001,87.83,7.65,3.91]], [2023-12-15T11:04:00Z,[1.07,10.37,99.27,16.78,0,57.15,0,910.85,0,0,0,25.92,25.92,0,0,16,1001,87.83,7.65,3.91]], [...\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Display the content of maxWeatherCode\
\
\cf3 scala> \cf2 val columnNamesMax = maxWeatherCode.schema.fieldNames\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 columnNamesMax
\f0\b0 \cf2 : 
\f1\b \cf5 Array[String]
\f0\b0 \cf2  = Array(_id, location, timelines)\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 val valuesMax = maxWeatherCode.toSeq\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 valuesMax
\f0\b0 \cf2 : 
\f1\b \cf5 Seq[Any]
\f0\b0 \cf2  = WrappedArray([657c31b2b8dc5929a778b295], [12.9716,77.5946], [WrappedArray([2023-12-15T10:59:00Z,[0.55,0.55,75.0,19.69,0,78.0,0,910.81,0,0,0,25.81,25.81,0,0,16,1001,96.13,3.19,2.13]], [2023-12-15T11:00:00Z,[1.07,10.37,99.22,16.72,0,56.45,0,910.81,0,0,0,26.04,26.04,0,0,16,1001,87.83,7.64,3.88]], [2023-12-15T11:01:00Z,[1.07,10.37,99.23,16.73,0,56.63,0,910.82,0,0,0,26.01,26.01,0,0,16,1001,87.83,7.64,3.89]], [2023-12-15T11:02:00Z,[1.07,10.37,99.24,16.75,0,56.8,0,910.83,0,0,0,25.98,25.98,0,0,16,1001,87.83,7.65,3.9]], [2023-12-15T11:03:00Z,[1.07,10.37,99.26,16.76,0,56.98,0,910.84,0,0,0,25.95,25.95,0,0,16,1001,87.83,7.65,3.91]], [2023-12-15T11:04:00Z,[1.07,10.37,99.27,16.78,0,57.15,0,910.85,0,0,0,25.92,25.92,0,0,16,1001,87.83,7.65,3.91]], [2023-12...\
\
=========\
Ml\
=======\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 import org.apache.spark.ml.regression.RandomForestRegressor\
import org.apache.spark.ml.regression.RandomForestRegressor\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Assuming df is your DataFrame\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Extracting features from the nested structure\
\
\cf3 scala> \cf2 val extractedData = weatherData.select(\
\cf3      | \cf2   "location.lat",\
\cf3      | \cf2   "location.lon",\
\cf3      | \cf2   "timelines.minutely.values.cloudBase",\
\cf3      | \cf2   "timelines.minutely.values.cloudCover",\
\cf3      | \cf2   "timelines.minutely.values.dewPoint",\
\cf3      | \cf2   // Add more features as needed\
\cf3      | \cf2 )\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 extractedData
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [lat: double, lon: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Renaming columns for simplicity\
\
\cf3 scala> \cf2 val renamedData = extractedData\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 renamedData
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [lat: double, lon: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .withColumnRenamed("lat", "latitude")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res390
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [latitude: double, lon: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .withColumnRenamed("lon", "longitude")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res391
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [latitude: double, longitude: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .withColumnRenamed("cloudBase", "cloudBaseMinutely")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res392
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [latitude: double, longitude: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .withColumnRenamed("cloudCover", "cloudCoverMinutely")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res393
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [latitude: double, longitude: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .withColumnRenamed("dewPoint", "dewPointMinutely")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res394
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.DataFrame
\f0\b0 \cf2  = [latitude: double, longitude: double ... 3 more fields]\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   // Rename more columns as needed\
\
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Assemble features into a single vector column\
\
\cf3 scala> \cf2 val featureCols = Array("latitude", "longitude", "cloudBaseMinutely", "cloudCoverMinutely", "dewPointMinutely")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 featureCols
\f0\b0 \cf2 : 
\f1\b \cf5 Array[String]
\f0\b0 \cf2  = Array(latitude, longitude, cloudBaseMinutely, cloudCoverMinutely, dewPointMinutely)\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 val assembler = new VectorAssembler()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 assembler
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.ml.feature.VectorAssembler
\f0\b0 \cf2  = VectorAssembler: uid=vecAssembler_0cd4819df27a, handleInvalid=error\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .setInputCols(featureCols)\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res395
\f0\b0 \cf2 : 
\f1\b \cf5 assembler.type
\f0\b0 \cf2  = VectorAssembler: uid=vecAssembler_0cd4819df27a, handleInvalid=error, numInputCols=5\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .setOutputCol("features")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res396
\f0\b0 \cf2 : 
\f1\b \cf5 res395.type
\f0\b0 \cf2  = VectorAssembler: uid=vecAssembler_0cd4819df27a, handleInvalid=error, numInputCols=5\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Create a RandomForestRegressor\
\
\cf3 scala> \cf2 val rf = new RandomForestRegressor()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 rf
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.ml.regression.RandomForestRegressor
\f0\b0 \cf2  = rfr_411001424378\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .setLabelCol("your_target_column")  // Replace with your actual target column\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res397
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.ml.regression.RandomForestRegressor
\f0\b0 \cf2  = rfr_411001424378\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .setFeaturesCol("features")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res398
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.ml.regression.RandomForestRegressor
\f0\b0 \cf2  = rfr_411001424378\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 // Create a pipeline\
\
\cf3 scala> \cf2 val pipeline = new Pipeline()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 pipeline
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.ml.Pipeline
\f0\b0 \cf2  = pipeline_a0c066d4a33b\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2   .setStages(Array(assembler, rf))\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 res399
\f0\b0 \cf2 : 
\f1\b \cf5 pipeline.type
\f0\b0 \cf2  = pipeline_a0c066d4a33b\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf3 scala> \cf2 \
\
\cf3 scala> \cf2 val Array(trainingData, testData) = renamedData.randomSplit(Array(0.8, 0.2))\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b \cf4 trainingData
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
\f0\b0 \cf2  = [lat: double, lon: double ... 3 more fields]\

\f1\b \cf4 testData
\f0\b0 \cf2 : 
\f1\b \cf5 org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
\f0\b0 \cf2  = [lat: double, lon: double ... 3 more fields]\
\
\
\pard\pardeftab720\partightenfactor0

\f4\fs28 \cf10 \cb7 \expnd0\expndtw0\kerning0
\CocoaLigature1 // Train the model\
val model = pipeline.fit(trainingData)\
\
// Make predictions on the test set\
val predictions = model.transform(testData)\
\
// Evaluate your model (you may need to replace RegressionEvaluator with a suitable evaluator for your task)\
val evaluator = new RegressionEvaluator()\
  .setLabelCol("your_target_column")\
  .setPredictionCol("prediction")\
val rmse = evaluator.evaluate(predictions)\
\
// Print the RMSE (Root Mean Squared Error)\
println(s"Root Mean Squared Error (RMSE) on test data: $rmse")\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \cb1 \kerning1\expnd0\expndtw0 \CocoaLigature0 \
\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\
\pard\pardeftab720\partightenfactor0

\f2\fs28 \cf6 \cb7 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\partightenfactor0

\f0\fs22 \cf2 \cb1 \kerning1\expnd0\expndtw0 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
SVD\
\
\
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf11 scala> \cf2 import org.apache.spark.mllib.linalg.distributed.RowMatrix\
import org.apache.spark.mllib.linalg.distributed.RowMatrix\
\
\cf11 scala> \cf2 import org.apache.spark.mllib.linalg.\{Matrix, SingularValueDecomposition, Vectors\}\
import org.apache.spark.mllib.linalg.\{Matrix, SingularValueDecomposition, Vectors\}\
\
\cf11 scala> \cf2 import org.apache.spark.sql.SparkSession\
import org.apache.spark.sql.SparkSession\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // Create a Spark session\
\
\cf11 scala> \cf2 val spark = SparkSession.builder.appName("SVDExample").getOrCreate()\

\f1\b \cf12 spark
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.sql.SparkSession
\f0\b0 \cf2  = org.apache.spark.sql.SparkSession@446de37d\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // Sample data (you should replace this with your own data)\
\
\cf11 scala> \cf2 val data = Seq(\
\cf11      | \cf2   Vectors.dense(1.0, 2.0, 3.0),\
\cf11      | \cf2   Vectors.dense(4.0, 5.0, 6.0),\
\cf11      | \cf2   Vectors.dense(7.0, 8.0, 9.0)\
\cf11      | \cf2 )\

\f1\b \cf12 data
\f0\b0 \cf2 : 
\f1\b \cf13 Seq[org.apache.spark.mllib.linalg.Vector]
\f0\b0 \cf2  = List([1.0,2.0,3.0], [4.0,5.0,6.0], [7.0,8.0,9.0])\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // Create an RDD of vectors\
\
\cf11 scala> \cf2 val rows = spark.sparkContext.parallelize(data)\

\f1\b \cf12 rows
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector]
\f0\b0 \cf2  = ParallelCollectionRDD[13] at parallelize at <console>:44\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // Create a RowMatrix\
\
\cf11 scala> \cf2 val rowMatrix = new RowMatrix(rows)\

\f1\b \cf12 rowMatrix
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.mllib.linalg.distributed.RowMatrix
\f0\b0 \cf2  = org.apache.spark.mllib.linalg.distributed.RowMatrix@29f47269\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // Compute SVD\
\
\cf11 scala> \cf2 val svd: SingularValueDecomposition[RowMatrix, Matrix] = rowMatrix.computeSVD(3, computeU = true)\

\f1\b \cf12 svd
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mllib.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix]
\f0\b0 \cf2  =\
SingularValueDecomposition(org.apache.spark.mllib.linalg.distributed.RowMatrix@6af02ef3,[16.848103352614206,1.0683695145547043,8.527732601206366E-8],-0.4796711778777719  0.7766909903215564   0.4082482904638688\
-0.5723677939720624  0.07568647010456442  -0.8164965809277254\
-0.6650644100663532  -0.6253180501124457  0.4082482904638585   )\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // U, S, and V are the result of SVD\
\
\cf11 scala> \cf2 val U: RowMatrix = svd.U\

\f1\b \cf12 U
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.mllib.linalg.distributed.RowMatrix
\f0\b0 \cf2  = org.apache.spark.mllib.linalg.distributed.RowMatrix@6af02ef3\
\
\cf11 scala> \cf2 val s: org.apache.spark.mllib.linalg.Vector = svd.s\

\f1\b \cf12 s
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.mllib.linalg.Vector
\f0\b0 \cf2  = [16.848103352614206,1.0683695145547043,8.527732601206366E-8]\
\
\cf11 scala> \cf2 val V: Matrix = svd.V\

\f1\b \cf12 V
\f0\b0 \cf2 : 
\f1\b \cf13 org.apache.spark.mllib.linalg.Matrix
\f0\b0 \cf2  =\
-0.4796711778777719  0.7766909903215564   0.4082482904638688\
-0.5723677939720624  0.07568647010456442  -0.8164965809277254\
-0.6650644100663532  -0.6253180501124457  0.4082482904638585\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 // Print the results\
\
\cf11 scala> \cf2 println("U:")\
U:\
\
\cf11 scala> \cf2 U.rows.foreach(println)\
[-0.8263375405610782,0.38794278236977675,5.960464477539063E-8]\
[-0.21483723836839635,-0.8872306883463738,-7.636845111846924E-8]\
[-0.5205873894647374,-0.24964395298829833,-7.450580596923828E-9]\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 println("Singular values:")\
Singular values:\
\
\cf11 scala> \cf2 s.toArray.foreach(println)\
16.848103352614206\
1.0683695145547043\
8.527732601206366E-8\
\
\cf11 scala> \cf2 \
\
\cf11 scala> \cf2 println("V:")\
V:\
\
\cf11 scala> \cf2 println(V)\
-0.4796711778777719  0.7766909903215564   0.4082482904638688   \
-0.5723677939720624  0.07568647010456442  -0.8164965809277254  \
-0.6650644100663532  -0.6253180501124457  0.4082482904638585   \
\
\cf11 scala> \cf2 \
\
\
\
\
\
\
====================\
import org.apache.spark.sql.functions._\
import org.apache.spark.sql.expressions.Window\
\
// Explode the "minutely" array to have each minute as a separate row\
val explodedDF = weatherData.select(\
  col("_id.oid").alias("oid"),\
  col("location.lat").alias("lat"),\
  col("location.lon").alias("lon"),\
  explode(col("timelines.minutely")).as("minute")\
)\
\
// Extract relevant fields for wind speed analysis\
val windSpeedDF = explodedDF.select(\
  col("oid"),\
  col("lat"),\
  col("lon"),\
  col("minute.time").alias("minute"),\
  col("minute.values.windSpeed").alias("windSpeed")\
)\
\
// Analyze wind speed variations for each minute\
val windowSpec = Window.partitionBy("oid", "lat", "lon").orderBy("minute")\
\
val resultDF = windSpeedDF.withColumn(\
  "windSpeedVariation",\
  col("windSpeed") - lag("windSpeed", 1).over(windowSpec)\
)\
\
// Show the result\
resultDF.show()\
\
// Stop the Spark session\
spark.stop()\
==================\
\
\
\
import org.apache.spark.sql.functions._\
import org.apache.spark.sql.expressions.Window\
val explodedDF = weatherData.select(\
  col("_id.oid").alias("oid"),\
  col("location.lat").alias("lat"),\
  col("location.lon").alias("lon"),\
  explode(col("timelines.minutely")).as("minute")\
)\
val analysisDF = explodedDF.select(\
  col("oid"),\
  col("lat"),\
  col("lon"),\
  col("minute.time").alias("minute"),\
  col("minute.values.windSpeed").alias("windSpeed"),\
  col("minute.values.visibility").alias("visibility")\
)\
val windowSpec = Window.partitionBy("oid", "lat", "lon").orderBy("minute")\
\
val resultDF = analysisDF.withColumn(\
  "windSpeedVariation",\
  col("windSpeed") - lag("windSpeed", 1).over(windowSpec)\
)\
val rainyData = analysisDF.filter("visibility > 0")\
val averageVisibilityDuringRain = rainyData.agg(avg("visibility").alias("avgVisibilityDuringRain")).first().getDouble(0)\
resultDF.show()\
println(s"Average visibility during rain: $averageVisibilityDuringRain meters")\
\
spark.stop()\
\
\
\
\
=====================\
import org.apache.spark.sql.functions._\
import org.apache.spark.sql.expressions.Window\
val explodedDF = weatherData.select(\
  col("_id.oid").alias("oid"),\
  col("location.lat").alias("lat"),\
  col("location.lon").alias("lon"),\
  explode(col("timelines.minutely")).as("minute")\
)\
val analysisDF = explodedDF.select(\
  col("oid"),\
  col("lat"),\
  col("lon"),\
  col("minute.time").alias("minute"),\
  col("minute.values.temperature").alias("temperature")\
)\
val windowSpec = Window.partitionBy("oid", "lat", "lon").orderBy("minute")\
\
val resultDF = analysisDF.withColumn(\
  "temperatureVariation",\
  col("temperature") - lag("temperature", 1).over(windowSpec)\
)\
val rainyData = analysisDF.filter("temperature > 0")\
val averageTemperatureDuringRain = rainyData.agg(avg("temperature").alias("avgTemperatureDuringRain")).first().getDouble(0)\
\
resultDF.show()\
println(s"Average temperature during rain: $averageTemperatureDuringRain degrees Celsius")\
spark.stop()\
================\
\
}